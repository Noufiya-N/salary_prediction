# -*- coding: utf-8 -*-
"""HR Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NVPRp4pLJY46kpolLezVWw6Ud1CEkFPA
"""

# Commented out IPython magic to ensure Python compatibility.
#Here we are going to find out what contributes to employees leaving the company

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

dataset = pd.read_csv('HR_comma_sep.csv')
data = dataset.copy()
data.info()

data.head()

#You will notice that there is an error with the column sales, I was expecting the column "department".
#Maybe it was accidentally renamed. Let's see what is in the "sales" column.

sales = data.groupby(by='sales').count()
sales

#I was right, it was renamed. so lets name it "department" now.

data.rename(columns={'sales': 'department'}, inplace=True)
data.head()

#Notice in the above table that salary column contains strings(high, medium, low).
#Let's add another column('Salary2') to the table that will return a number if salary is high(3), medium(2) or low(1).
#and check the describe command.

def salary(row):
    if row['salary'] == 'high':
        return 3
    elif row['salary'] == 'medium':
        return 2
    else:
        return 1

data['Salary2'] = data.apply(salary, axis=1)

data.describe()

data_group = data.groupby(by=['department'],as_index=False).count()
ax = sns.barplot(x="department", y="left", data=data_group)
plt.xticks(rotation = 90)

# Step 2: Clean column names (if necessary)
data.columns = data.columns.str.strip()

# Step 3: Convert categorical variables to numerical
# You can choose to drop non-numeric columns if they are not needed
# or convert them using one-hot encoding.
data_encoded = pd.get_dummies(data, drop_first=True)

# Step 4: Calculate the correlation matrix
corr = data_encoded.corr()

# Step 5: Create a mask for the upper triangle
mask = np.zeros_like(corr, dtype=bool)
mask[np.triu_indices_from(mask)] = True

# Step 6: Set up the matplotlib figure
plt.figure(figsize=(13, 12))

# Step 7: Draw the heatmap
sns.heatmap(corr, mask=mask, vmax=.3, center=0, annot=True,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

plt.title('Correlation Heatmap')

data['salary'].value_counts()

#We See that Sales, Support and Technical Departments have more employees who left the company.
#However, without knowing the total staffing count at the organization,
#this variable isn't too helpful.
#Before we go any further, let's see what is correlated in the dataset.

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="white")

# Convert 'department' and 'salary' columns to numerical using Label Encoding
# before calculating correlation
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
data['department_encoded'] = label_encoder.fit_transform(data['department'])
data['salary_encoded'] = label_encoder.fit_transform(data['salary'])


# Now calculate the correlation matrix using only numerical features
corr = data[['satisfaction_level', 'last_evaluation', 'number_project',
            'average_montly_hours', 'time_spend_company', 'Work_accident',
            'left', 'promotion_last_5years', 'Salary2', 'department_encoded',
            'salary_encoded']].corr()

# Use 'bool' instead of 'np.bool'
mask = np.zeros_like(corr, dtype=bool)
mask[np.triu_indices_from(mask)] = True

vf, ax = plt.subplots(figsize=(11, 9))

sns.heatmap(corr, mask=mask, vmax=.3, center=0,annot=True,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

#This correlation matrix tells us that the higher someone is satisfied in their job the less likely they are to leave.
#Time spent with the company is positively correlated with leaving,
#suggesting the longer someone works for this organization the more likely they are too leave.
#Another relationship that makes sense is that a higher salary appears to be negatively associated with leaving.
#One correlation in the above matrix doesn't quite make sense.
#Having had a work accident is negatively related to leaving the company, which doesn't really make intuitive sense.
#On the surface this doesn't make a whole lot of sense
#because you would assume that an employee felt unsafe at an organization that they might want to leave.
#For now, let's look closer at the satisfaction level variable, the one with the highest coefficient.

ax = sns.distplot(data['satisfaction_level'])

#Satisfaction is definitely not normally distributed.
#You could group the data for additional analyses is based on the histogram above is where the
#bars take a distinctive drop-off.

#I'm going to create a new column and assign qualitative rankings for three(3) Satisfaction levels(High, Medium and Low).
#and add a new column "Sat" to hold the qualitative rankings.

def sat(row):
    if .3 <= row['satisfaction_level'] < .5:
        return 'Medium'
    elif row['satisfaction_level'] >= .5:
        return 'High'
    else:
        return 'Low'

data['Sat'] = data.apply(sat, axis=1)

# Specify numeric_only=True to only calculate the mean for numeric columns
data_gr = data.groupby(by='Sat', as_index=False).mean(numeric_only=True)
data_gr.head()

#lets use the Sa


data_group = data.groupby(by=['Sat'])
left_rate = data_group['left'].sum() / data_group['left'].count()
ax = left_rate.plot(kind='barh')

#Well that's interesting.
#Those with medium satisfaction levels are more likely to leave based on percentage of those who left
#if we use our three-category breakdown.
#This chart suggests that those who are highly satisfied are the least likely to leave, which makes intuitive sense.

#Now let's check out salary, which is an ordinal, string variable in the original dataset.

data_group = data.groupby(by=['salary'],as_index=False).count()
data_group

# We see that most people are in the low or medium salary categories.
#Let's look at the pure count of folks who left in each category.

ax = sns.barplot(x="salary", y="left", data=data_group)

#Now let's look at those who left the company by calculating the percentage or rate.

data_group = data.groupby(by=['salary'])
left_rate = data_group['left'].sum() / data_group['left'].count()
ax = left_rate.plot(kind='barh')

#Salary appears to have a a linear relationship with leaving,
#with the more money your earn the less likely you are to leave.
#But, let's see if initial analysis suggest any potential interaction effects between
#salary and satisfaction in terms of leaving the employer.

# Set a default value
data_group = data.groupby(by=['salary','Sat'])
left_rate = data_group['left'].sum() / data_group['left'].count()
ax = left_rate.plot(kind='barh')

#Findings:
#Employees with the highest rate of leaving earns a low salary and reports a medium satisfaction rate.
#Aso Employees with the lowest rate of leaving earns the highest salary and reports the highest satsifaction.

data.isna().sum()

# prompt: check the outlier of each coloumn

# Assuming your data is in a DataFrame called 'data'
def find_outliers_iqr(data):
  """
  Find outliers in each column of a DataFrame using the IQR method.

  Args:
    data: A pandas DataFrame.

  Returns:
    A dictionary where keys are column names and values are lists of outlier values.
  """

  outliers = {}
  for column in data.select_dtypes(include=np.number).columns:
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers[column] = [x for x in data[column] if x < lower_bound or x > upper_bound]

  return outliers

# Find outliers for numerical columns
outliers_iqr = find_outliers_iqr(data)

# Print outliers for each column
for column, outlier_values in outliers_iqr.items():
    print(f"Outliers in {column}: {outlier_values}")

# prompt: make the boxplot of outlier column   of each column

import matplotlib.pyplot as plt

# Assuming you have a dictionary 'outliers_iqr' with outliers for each column
# as obtained in your previous code.

for column, outlier_values in outliers_iqr.items():
  if outlier_values:  # Check if there are any outliers for this column
    plt.figure()  # Create a new figure for each column
    plt.boxplot(data[column])
    plt.title(f"Boxplot of {column} with Outliers")
    plt.ylabel(column)
    plt.show()

# prompt: remove outlier

# Assuming your data is in a DataFrame called 'data'
def remove_outliers_iqr(data):
  """
  Remove outliers in each column of a DataFrame using the IQR method.

  Args:
    data: A pandas DataFrame.

  Returns:
    A new DataFrame with outliers removed.
  """

  data_copy = data.copy()  # Create a copy to avoid modifying the original DataFrame

  for column in data_copy.select_dtypes(include=np.number).columns:
    Q1 = data_copy[column].quantile(0.25)
    Q3 = data_copy[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data_copy = data_copy[(data_copy[column] >= lower_bound) & (data_copy[column] <= upper_bound)]

  return data_copy


# Remove outliers for numerical columns
data_no_outliers = remove_outliers_iqr(data)

# Check if outliers have been removed
print(f"Original shape: {data.shape}")
print(f"Shape after removing outliers: {data_no_outliers.shape}")

# You can now use data_no_outliers for further analysis with outliers removed.

# prompt: make the boxplot of outlier column   of each column

import matplotlib.pyplot as plt

# Assuming you have a dictionary 'outliers_iqr' with outliers for each column
# as obtained in your previous code.

for column, outlier_values in outliers_iqr.items():
  if outlier_values:  # Check if there are any outliers for this column
    plt.figure()  # Create a new figure for each column
    plt.boxplot(data[column])
    plt.title(f"Boxplot of {column} with Outliers")
    plt.ylabel(column)
    plt.show()

# prompt: make the boxplot of outlier column   of each column

import matplotlib.pyplot as plt

# Assuming you have a dictionary 'outliers_iqr' with outliers for each column
# as obtained in your previous code.

for column, outlier_values in outliers_iqr.items():
  if outlier_values:  # Check if there are any outliers for this column
    plt.figure()  # Create a new figure for each column
    plt.boxplot(data[column])
    plt.title(f"Boxplot of {column} with Outliers")
    plt.ylabel(column)
    plt.show()

outliers =['time_spend_company','promotion_last_5years','Work_accident','left']

for column in outliers:
  Q1=dataset[column].quantile(0.25)
  Q3=dataset[column].quantile(0.75)
  IQR=Q3-Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  dataset[column] = np.clip(dataset[column],lower_bound,upper_bound)

for col in outliers:
  sns.boxplot(dataset[col])
  plt.show()

# Bell Curve Plotting using 'salary' column in Jupyter Notebook

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns  # Optional, for better aesthetics

# Load the dataset
dataset = pd.read_csv('/content/HR_comma_sep.csv')  # Adjust the path as needed

# Check if the 'salary' column exists and drop missing values
if 'salary' in dataset.columns:
    # Convert 'salary' to numerical representation
    salary_mapping = {'low': 1, 'medium': 2, 'high': 3}  # Assign numerical values
    dataset['salary_numeric'] = dataset['salary'].map(salary_mapping)  # Create a new numeric column
    data = dataset['salary_numeric'].dropna()  # Use the new numeric column
else:
    raise ValueError("The 'salary' column does not exist in the dataset.")

# Calculate mean and standard deviation
mu, std = np.mean(data), np.std(data)

# Create a range of x values
x = np.linspace(mu - 4*std, mu + 4*std, 1000)  # 4 standard deviations from the mean

# Calculate the probability density function (PDF)
pdf = (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / std) ** 2)

# Plotting the bell curve
plt.figure(figsize=(10, 6))
plt.plot(x, pdf, color='blue', label='Bell Curve', linewidth=2)
plt.hist(data, bins=30, density=True, alpha=0.5, color='gray', label='Histogram of salary')
plt.title('Bell Curve of  salary')
plt.xlabel('salary')
plt.ylabel('Probability Density')
plt.legend()
plt.grid()
plt.show()

from sklearn.preprocessing import MinMaxScaler

# Select only numeric columns
numeric_columns = dataset.select_dtypes(include=[np.number]).columns  # Get numeric column names

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit the scaler on the numeric columns and transform them
dataset[numeric_columns] = scaler.fit_transform(dataset[numeric_columns])

# Optionally, print the scaled DataFrame
print(dataset[numeric_columns].head())

# If there are any categorical variables, use label encoding or one-hot encoding
dataset = pd.get_dummies(dataset, drop_first=True)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
data = pd.read_csv('/content/HR_comma_sep.csv')

# Prepare the data
X = data[['time_spend_company','promotion_last_5years','Work_accident','left']]  # Replace with your feature columns
y = data['salary']  # Replace with your target column

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)

# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)

# R² Score
r2 = r2_score(y_test, y_pred)

# Print the evaluation results
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R² Score: {r2}")

from sklearn.model_selection import GridSearchCV

# Initialize Random Forest model
model = RandomForestRegressor(random_state=42)

# Define parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'max_features': ['sqrt', None]  # Use 'sqrt' or None, not 'auto'
}

# Initialize Grid Search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1)

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best parameters and best R² score
print("Best Parameters:", grid_search.best_params_)
print("Best R² Score:", grid_search.best_score_)

from sklearn.ensemble import RandomForestRegressor

# Initialize the Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model on the training data
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder  # Import OneHotEncoder

# Create a OneHotEncoder
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # sparse=False for dense output

# Fit the encoder on the categorical features and transform them
encoded_features = encoder.fit_transform(X.loc[:,['sales']])  # Only apply to 'sales' column


# Create a DataFrame from the encoded features
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['sales']))  # Prefix column names with 'sales_'

#

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
data = pd.read_csv('/content/HR_comma_sep.csv')

# Check the unique values in the salary column
print("Unique values in the salary column:", data['salary'].unique())

# Map categorical salary values to numeric values
salary_mapping = {
    'low': 0,
    'medium': 1,
    'high': 2
}
data['salary'] = data['salary'].map(salary_mapping)

# Prepare the data
X = data[['time_spend_company', 'promotion_last_5years', 'Work_accident', 'left']]  # Replace with your feature columns
y = data['salary']  # This should now be numeric

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
data = pd.read_csv('/content/HR_comma_sep.csv')

# Check the unique values in the salary column
print("Unique values in the salary column:", data['salary'].unique())

# Map categorical salary values to numeric values
salary_mapping = {
    'low': 0,
    'medium': 1,
    'high': 2
}
data['salary'] = data['salary'].map(salary_mapping)

# Prepare the data
X = data[['time_spend_company', 'promotion_last_5years', 'Work_accident', 'left']]  # Replace with your feature columns
y = data['salary']  # This should now be numeric

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Example for making a prediction
sample_input = pd.DataFrame({
    'time_spend_company': [1.0],
    'promotion_last_5years': [0],
    'Work_accident': [1],
    'left': [0]  # Assuming 'left' is a binary column where 0 means not left and 1 means left
})

# Make a prediction for the sample input
sample_prediction = model.predict(sample_input)

# Convert the predicted numeric value back to the original categories
predicted_value = sample_prediction[0]

if predicted_value < 0.5:
    predicted_category = 'low'
elif 0.5 <= predicted_value < 1.5:
    predicted_category = 'medium'
else:
    predicted_category = 'high'

print(f"Predicted Salary Category for the sample: {predicted_category}")

from sklearn.model_selection import GridSearchCV

# Define the model
model = RandomForestRegressor(random_state=42)

# Set the parameters for Grid Search
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Initialize Grid Search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='r2')

# Fit Grid Search
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
print("Best Parameters:", grid_search.best_params_)
print("Best R-squared:", grid_search.best_score_)

numpy as np
import pandas asimport pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder

# Load dataset
data = pd.read_csv('/content/HR_comma_sep.csv')

# Prepare the data
X = data[['time_spend_company','promotion_last_5years','Work_accident','left']]  # Replace with your feature columns
y = data['salary']  # Replace with your target column

# Create a LabelEncoder
label_encoder = LabelEncoder()

# Fit the encoder to the target variable and transform it
y_encoded = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Defining the feature set (X) using the specified columns
X = dataset[['time_spend_company','promotion_last_5years','Work_accident','left']]  # Selected features
y = dataset['salary']  # Target variable (salary)

# Splitting the data into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Random Forest Regressor model
model = RandomForestRegressor(random_state=42)

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Optional: Print the first few predictions vs actual values
print("first 5 Predictions:", y_pred[:5])
print("First 5 Actual Prices:", y_test[:5].values)

# Calculate and print evaluation metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

print("first 5 Predictions:", y_pred[:5])
print("First 5 Actual Prices:", y_test[:5])  # Remove '.values'

import pandas as pd

# Define the feature values for a sample input
sample_input = pd.DataFrame({
    'time_spend_company': [1.28995952],
    'promotion_last_5years':[1.33954789 ],
    'Work_accident': [1.33954789],
    'left': [1.31967448]
})

# Make a prediction for the sample input
sample_prediction = model.predict(sample_input)

# Print the predicted price for the sample
print(f"Predicted salary for the sample: {sample_prediction[0]}")

import pandas as pd

# Define the feature values for a sample input (including 'left')
sample_input = pd.DataFrame({
    'time_spend_company': [1.0],           # Example value for time spent in the company
    'promotion_last_5years': [0],          # Example value for promotion in the last 5 years
    'Work_accident': [1],                   # Example value for work accident occurrence
    'left': [0]                             # Example value for 'left' (0 or 1 depending on the scenario)
})

# Make a prediction for the sample input
sample_prediction = model.predict(sample_input)

# Print the predicted salary for the sample
print(f"Predicted Salary for the sample: {sample_prediction[0]}")

import pandas as pd

# Load the dataset
data = pd.read_csv('/content/HR_comma_sep.csv')

# Check the target variable for unique values
print("Unique values in the target variable:")
print(data['salary'].unique())

# If 'salary' is categorical, map it to numerical values
# For example, if salary contains 'low', 'medium', 'high', map them to 0, 1, 2
if data['salary'].dtype == 'object':
    salary_mapping = {
        'low': 0,
        'medium': 1,
        'high': 2
    }
    data['salary'] = data['salary'].map(salary_mapping)

# Now check if there are any NaN values after mapping
print("Checking for NaN values after mapping:")
print(data['salary'].isna().sum())

# Drop rows with NaN in 'salary' if any exist
data = data.dropna(subset=['salary'])

# Now select features and target variable
X = data[['time_spend_company', 'promotion_last_5years', 'Work_accident']]
y = data['salary']  # This should now be numeric

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Define the feature values for a sample input
sample_input = pd.DataFrame({
    'time_spend_company': [1.0],           # Example value for time spent in the company
    'promotion_last_5years': [0],          # Example value for promotion in the last 5 years
    'Work_accident': [1]                   # Example value for work accident occurrence
})

# Make a prediction for the sample input
sample_prediction = model.predict(sample_input)

# Print the predicted salary for the sample
print(f"Predicted Salary for the sample: {sample_prediction[0]}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# Load the dataset
data = pd.read_csv('/content/HR_comma_sep.csv')

# Check and map the target variable
salary_mapping = {
    'low': 0,
    'medium': 1,
    'high': 2
}
data['salary'] = data['salary'].map(salary_mapping)

# Select features and target variable
X = data[['time_spend_company', 'promotion_last_5years', 'Work_accident']]
y = data['salary']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Define the feature values for a sample input
sample_input = pd.DataFrame({
    'time_spend_company': [1.0],
    'promotion_last_5years': [0],
    'Work_accident': [1]
})

# Make a prediction for the sample input
sample_prediction = model.predict(sample_input)

# Print the predicted salary for the sample
predicted_value = sample_prediction[0]

# Convert the predicted numeric value back to the original categories
if predicted_value < 0.5:
    predicted_category = 'low'
elif 0.5 <= predicted_value < 1.5:
    predicted_category = 'medium'
else:
    predicted_category = 'high'

print(f"Predicted Salary Category for the sample: {predicted_category}")

